This is the output from testing one run of the gemm implementation of vgg16, I removed the part of the code that forces the layers into a GPU
This does not run on the gpu, because ai3 runs it on the CPU

(ai3_env) [wxs428@gput050 gemm]$ python python.py 
================================================================================
VGG16 GEMM IMPLEMENTATION WITH AI3 LIBRARY
================================================================================
CUDA Device Information:
  ✓ CUDA is available
  ✓ CUDA version: 12.4
  ✓ Number of GPUs: 1
  ✓ Current GPU: NVIDIA GeForce RTX 2080 Ti
  ✓ GPU Memory: 10.6 GB
  ✓ CUDA context initialized successfully

Configuration:
  Model: VGG16
  Algorithm: gemm
  Device: cuda
  Batch size: 1
  Iterations per input size: 10
  Input sizes to test: 1

Loading VGG16 model...
✓ VGG16 model loaded successfully

Original model loaded. Analyzing structure...
Found 13 Conv2D layers in original model

Applying AI3 gemm algorithm conversion...
✓ AI3 conversion completed successfully

============================================================
MODEL STRUCTURE ANALYSIS
============================================================
✓ AI3 Layer: features.0
  - Algorithm: gemm
  - Weight shape: torch.Size([64, 3, 3, 3])
✓ AI3 Layer: features.2
  - Algorithm: gemm
  - Weight shape: torch.Size([64, 64, 3, 3])
✓ AI3 Layer: features.5
  - Algorithm: gemm
  - Weight shape: torch.Size([128, 64, 3, 3])
✓ AI3 Layer: features.7
  - Algorithm: gemm
  - Weight shape: torch.Size([128, 128, 3, 3])
✓ AI3 Layer: features.10
  - Algorithm: gemm
  - Weight shape: torch.Size([256, 128, 3, 3])
✓ AI3 Layer: features.12
  - Algorithm: gemm
  - Weight shape: torch.Size([256, 256, 3, 3])
✓ AI3 Layer: features.14
  - Algorithm: gemm
  - Weight shape: torch.Size([256, 256, 3, 3])
✓ AI3 Layer: features.17
  - Algorithm: gemm
  - Weight shape: torch.Size([512, 256, 3, 3])
✓ AI3 Layer: features.19
  - Algorithm: gemm
  - Weight shape: torch.Size([512, 512, 3, 3])
✓ AI3 Layer: features.21
  - Algorithm: gemm
  - Weight shape: torch.Size([512, 512, 3, 3])
✓ AI3 Layer: features.24
  - Algorithm: gemm
  - Weight shape: torch.Size([512, 512, 3, 3])
✓ AI3 Layer: features.26
  - Algorithm: gemm
  - Weight shape: torch.Size([512, 512, 3, 3])
✓ AI3 Layer: features.28
  - Algorithm: gemm
  - Weight shape: torch.Size([512, 512, 3, 3])

============================================================
CONVERSION SUMMARY:
  Total Conv2D layers: 13
  AI3 converted: 13
  PyTorch remaining: 0
  Conversion rate: 100.0%
============================================================


Analyzing device placement after AI3 conversion...
✓ AI3 device distribution:
  - Parameters on CUDA: 0/32
  - Parameters on CPU: 32/32
  - Mixed device usage is expected with AI3
  - AI3 layers on CUDA: 0
  - AI3 layers on CPU: 13

Starting performance testing...
This will test the model with various input sizes to measure GEMM performance.

[1/1] Testing with input size 282x282
  ✓ Created input tensor: torch.Size([1, 3, 282, 282]) on cpu
  GPU Memory - Allocated: 0.00 GB, Reserved: 0.00 GB
  Running warmup...
  ✓ Warmup completed
  GPU Memory after warmup - Allocated: 0.00 GB, Reserved: 0.00 GB
  Measuring performance over 10 iterations...
    Iteration 1/10
    Iteration 6/10
  ✓ Average execution time: 85.24 ms
  Top 5 slowest layers:
    features.2: 8.56 ms (10.0%) [gemm]
    features.0: 4.82 ms (5.7%) [gemm]
    features.7: 4.51 ms (5.3%) [gemm]
    features.19: 4.30 ms (5.0%) [gemm]
    features.21: 4.24 ms (5.0%) [gemm]

================================================================================
VGG16 GEMM TESTING COMPLETED
================================================================================
Results saved to:
  - Overall performance: /home/wxs428/vgg16/gemm/VGG16_gemm_cuda_overall.csv
  - Layer-wise performance: /home/wxs428/vgg16/gemm/VGG16_gemm_cuda_layers.csv

Summary:
  ✓ Tested 1 different input sizes
  ✓ 10 iterations per input size
  ✓ GEMM algorithm optimization using AI3 library
  ✓ Comprehensive layer-wise performance analysis

Next steps:
  1. Analyze the CSV files to compare GEMM performance vs standard convolution
  2. Run the same test with different algorithms (direct, smm, etc.) for comparison
  3. Test on GPU (set device='cuda') for hardware-accelerated GEMM operations
================================================================================
