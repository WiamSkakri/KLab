not supported by ai3

(ai3_env) [wxs428@gput068 googlenet]$ ls
direct_cpu  gemm  implicit-gemm  precomp-gemm  smm_cpu  winograd
(ai3_env) [wxs428@gput068 googlenet]$ cd precomp-gemm/
(ai3_env) [wxs428@gput068 precomp-gemm]$ ls
GoogleNet_implicit_precomp_gemm_cuda_layers.csv  GoogleNet_implicit_precomp_gemm_cuda_overall.csv  job.sh  python.py
(ai3_env) [wxs428@gput068 precomp-gemm]$ vim python.py 
(ai3_env) [wxs428@gput068 precomp-gemm]$ python python.py 
================================================================================
GOOGLENET IMPLICIT PRECOMPUTED GEMM IMPLEMENTATION WITH AI3 LIBRARY
================================================================================
CUDA Device Information:
  ✓ CUDA is available
  ✓ CUDA version: 12.4
  ✓ Number of GPUs: 1
  ✓ Current GPU: NVIDIA L40S
  ✓ GPU Memory: 44.4 GB
  ✓ CUDA context initialized successfully

Configuration:
  Model: GoogleNet
  Algorithm: implicit_precomp_gemm
  Device: cuda
  Batch size: 1
  Iterations per input size: 10
  Input sizes to test: 1

Loading GoogleNet model...
✓ GoogleNet model loaded successfully

Original model loaded. Analyzing structure...
Found 57 Conv2D layers in original model

Applying AI3 implicit_precomp_gemm algorithm conversion...
✗ Error during AI3 conversion: algorithm, implicit_precomp_gemm, is unsupported for conv2d
This might be due to AI3 library not being installed or configured properly.
Falling back to standard PyTorch implementation...

============================================================
MODEL STRUCTURE ANALYSIS
============================================================
⚠ PyTorch Layer: conv1.conv
  - Type: Conv2d
  - In/Out channels: 3/64
⚠ PyTorch Layer: conv2.conv
  - Type: Conv2d
  - In/Out channels: 64/64
⚠ PyTorch Layer: conv3.conv
  - Type: Conv2d
  - In/Out channels: 64/192
⚠ PyTorch Layer: inception3a.branch1.conv
  - Type: Conv2d
  - In/Out channels: 192/64
⚠ PyTorch Layer: inception3a.branch2.0.conv
  - Type: Conv2d
  - In/Out channels: 192/96
⚠ PyTorch Layer: inception3a.branch2.1.conv
  - Type: Conv2d
  - In/Out channels: 96/128
⚠ PyTorch Layer: inception3a.branch3.0.conv
  - Type: Conv2d
  - In/Out channels: 192/16
⚠ PyTorch Layer: inception3a.branch3.1.conv
  - Type: Conv2d
  - In/Out channels: 16/32
⚠ PyTorch Layer: inception3a.branch4.1.conv
  - Type: Conv2d
  - In/Out channels: 192/32
⚠ PyTorch Layer: inception3b.branch1.conv
  - Type: Conv2d
  - In/Out channels: 256/128
⚠ PyTorch Layer: inception3b.branch2.0.conv
  - Type: Conv2d
  - In/Out channels: 256/128
⚠ PyTorch Layer: inception3b.branch2.1.conv
  - Type: Conv2d
  - In/Out channels: 128/192
⚠ PyTorch Layer: inception3b.branch3.0.conv
  - Type: Conv2d
  - In/Out channels: 256/32
⚠ PyTorch Layer: inception3b.branch3.1.conv
  - Type: Conv2d
  - In/Out channels: 32/96
⚠ PyTorch Layer: inception3b.branch4.1.conv
  - Type: Conv2d
  - In/Out channels: 256/64
⚠ PyTorch Layer: inception4a.branch1.conv
  - Type: Conv2d
  - In/Out channels: 480/192
⚠ PyTorch Layer: inception4a.branch2.0.conv
  - Type: Conv2d
  - In/Out channels: 480/96
⚠ PyTorch Layer: inception4a.branch2.1.conv
  - Type: Conv2d
  - In/Out channels: 96/208
⚠ PyTorch Layer: inception4a.branch3.0.conv
  - Type: Conv2d
  - In/Out channels: 480/16
⚠ PyTorch Layer: inception4a.branch3.1.conv
  - Type: Conv2d
  - In/Out channels: 16/48
⚠ PyTorch Layer: inception4a.branch4.1.conv
  - Type: Conv2d
  - In/Out channels: 480/64
⚠ PyTorch Layer: inception4b.branch1.conv
  - Type: Conv2d
  - In/Out channels: 512/160
⚠ PyTorch Layer: inception4b.branch2.0.conv
  - Type: Conv2d
  - In/Out channels: 512/112
⚠ PyTorch Layer: inception4b.branch2.1.conv
  - Type: Conv2d
  - In/Out channels: 112/224
⚠ PyTorch Layer: inception4b.branch3.0.conv
  - Type: Conv2d
  - In/Out channels: 512/24
⚠ PyTorch Layer: inception4b.branch3.1.conv
  - Type: Conv2d
  - In/Out channels: 24/64
⚠ PyTorch Layer: inception4b.branch4.1.conv
  - Type: Conv2d
  - In/Out channels: 512/64
⚠ PyTorch Layer: inception4c.branch1.conv
  - Type: Conv2d
  - In/Out channels: 512/128
⚠ PyTorch Layer: inception4c.branch2.0.conv
  - Type: Conv2d
  - In/Out channels: 512/128
⚠ PyTorch Layer: inception4c.branch2.1.conv
  - Type: Conv2d
  - In/Out channels: 128/256
⚠ PyTorch Layer: inception4c.branch3.0.conv
  - Type: Conv2d
  - In/Out channels: 512/24
⚠ PyTorch Layer: inception4c.branch3.1.conv
  - Type: Conv2d
  - In/Out channels: 24/64
⚠ PyTorch Layer: inception4c.branch4.1.conv
  - Type: Conv2d
  - In/Out channels: 512/64
⚠ PyTorch Layer: inception4d.branch1.conv
  - Type: Conv2d
  - In/Out channels: 512/112
⚠ PyTorch Layer: inception4d.branch2.0.conv
  - Type: Conv2d
  - In/Out channels: 512/144
⚠ PyTorch Layer: inception4d.branch2.1.conv
  - Type: Conv2d
  - In/Out channels: 144/288
⚠ PyTorch Layer: inception4d.branch3.0.conv
  - Type: Conv2d
  - In/Out channels: 512/32
⚠ PyTorch Layer: inception4d.branch3.1.conv
  - Type: Conv2d
  - In/Out channels: 32/64
⚠ PyTorch Layer: inception4d.branch4.1.conv
  - Type: Conv2d
  - In/Out channels: 512/64
⚠ PyTorch Layer: inception4e.branch1.conv
  - Type: Conv2d
  - In/Out channels: 528/256
⚠ PyTorch Layer: inception4e.branch2.0.conv
  - Type: Conv2d
  - In/Out channels: 528/160
⚠ PyTorch Layer: inception4e.branch2.1.conv
  - Type: Conv2d
  - In/Out channels: 160/320
⚠ PyTorch Layer: inception4e.branch3.0.conv
  - Type: Conv2d
  - In/Out channels: 528/32
⚠ PyTorch Layer: inception4e.branch3.1.conv
  - Type: Conv2d
  - In/Out channels: 32/128
⚠ PyTorch Layer: inception4e.branch4.1.conv
  - Type: Conv2d
  - In/Out channels: 528/128
⚠ PyTorch Layer: inception5a.branch1.conv
  - Type: Conv2d
  - In/Out channels: 832/256
⚠ PyTorch Layer: inception5a.branch2.0.conv
  - Type: Conv2d
  - In/Out channels: 832/160
⚠ PyTorch Layer: inception5a.branch2.1.conv
  - Type: Conv2d
  - In/Out channels: 160/320
⚠ PyTorch Layer: inception5a.branch3.0.conv
  - Type: Conv2d
  - In/Out channels: 832/32
⚠ PyTorch Layer: inception5a.branch3.1.conv
  - Type: Conv2d
  - In/Out channels: 32/128
⚠ PyTorch Layer: inception5a.branch4.1.conv
  - Type: Conv2d
  - In/Out channels: 832/128
⚠ PyTorch Layer: inception5b.branch1.conv
  - Type: Conv2d
  - In/Out channels: 832/384
⚠ PyTorch Layer: inception5b.branch2.0.conv
  - Type: Conv2d
  - In/Out channels: 832/192
⚠ PyTorch Layer: inception5b.branch2.1.conv
  - Type: Conv2d
  - In/Out channels: 192/384
⚠ PyTorch Layer: inception5b.branch3.0.conv
  - Type: Conv2d
  - In/Out channels: 832/48
⚠ PyTorch Layer: inception5b.branch3.1.conv
  - Type: Conv2d
  - In/Out channels: 48/128
⚠ PyTorch Layer: inception5b.branch4.1.conv
  - Type: Conv2d
  - In/Out channels: 832/128

============================================================
CONVERSION SUMMARY:
  Total Conv2D layers: 57
  AI3 converted: 0
  PyTorch remaining: 57
  Conversion rate: 0.0%
============================================================


Analyzing device placement after AI3 conversion...
✓ AI3 device distribution:
  - Parameters on CUDA: 0/173
  - Parameters on CPU: 173/173
  - Mixed device usage is expected with AI3

Starting performance testing...
This will test the model with various input sizes to measure Implicit Precomputed GEMM performance.

[1/1] Testing with input size 440x440
  ✓ Created input tensor: torch.Size([1, 3, 440, 440]) on cpu
  GPU Memory - Allocated: 0.00 GB, Reserved: 0.00 GB
  Running warmup...
  ✓ Warmup completed
  GPU Memory after warmup - Allocated: 0.00 GB, Reserved: 0.00 GB
  Measuring performance over 10 iterations...
    Iteration 1/10
    Iteration 6/10
  ✓ Average execution time: 60.76 ms
  Top 5 slowest layers:
    conv3.conv: 3.94 ms (6.5%) [pytorch_conv2d]
    conv1.conv: 1.99 ms (3.3%) [pytorch_conv2d]
    inception3b.branch2.1.conv: 1.79 ms (3.0%) [pytorch_conv2d]
    inception3a.branch2.1.conv: 0.91 ms (1.5%) [pytorch_conv2d]
    inception4e.branch2.1.conv: 0.86 ms (1.4%) [pytorch_conv2d]

================================================================================
GOOGLENET IMPLICIT PRECOMPUTED GEMM TESTING COMPLETED
================================================================================
Results saved to:
  - Overall performance: /home/wxs428/googlenet/precomp-gemm/GoogleNet_implicit_precomp_gemm_cuda_overall.csv
  - Layer-wise performance: /home/wxs428/googlenet/precomp-gemm/GoogleNet_implicit_precomp_gemm_cuda_layers.csv

Summary:
  ✓ Tested 1 different input sizes
  ✓ 10 iterations per input size
  ✓ Implicit Precomputed GEMM algorithm optimization using AI3 library
  ✓ Comprehensive layer-wise performance analysis

Next steps:
  1. Analyze the CSV files to compare Implicit Precomputed GEMM performance vs standard convolution
  2. Run the same test with different algorithms (direct, gemm, smm, etc.) for comparison
  3. Test on GPU (set device='cuda') for hardware-accelerated GEMM operations
================================================================================
(ai3_env) [wxs428@gput068 precomp-gemm]$ 